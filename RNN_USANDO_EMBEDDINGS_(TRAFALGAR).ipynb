{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JMartinArocha/MasterBigData/blob/main/RNN_USANDO_EMBEDDINGS_(TRAFALGAR).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX8gZlVyCCbz"
      },
      "source": [
        "# Laboratorio: Modelos del lenguaje con RNNs\n",
        "\n",
        "En este laboratorio, vamos a entrenar un modelo del lenguaje basado en caracteres con Recurrent Neural Networks. Asimismo, utilizaremos el modelo para generar texto. En particular, alimentaremos nuestro modelo con obras de la literatura clásica en castellano para obtener una red neuronal que sea capaz de \"escribir\" fragmentos literarios.\n",
        "\n",
        "Los entrenamientos en esta laboratorio para obtener un modelo de calidad podrían tomar cierto tiempo (5-10 minutos por epoch), por lo que se aconseja empezar a trabajar pronto. El uso de GPUs no ayuda tanto con LSTMs como con CNNs, por lo que si tenéis máquinas potentes en casa es posible que podáis entrenar más rápido o a la misma velocidad que en Colab. En todo caso, la potencia de Colab es más que suficiente para completar este laboratorio con éxito.\n",
        "\n",
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Portada_Trafalgar_%281873%29.jpg/800px-Portada_Trafalgar_%281873%29.jpg\" style=\"text-align: center\" height=\"300px\"></center>\n",
        "\n",
        "El dataset a utilizar consistirá en un archivo de texto con el contenido íntegro en castellano de Trafalgar, disponible de manera libre en la página de [Project Gutenberg](https://www.gutenberg.org). Asimismo, como apartado optativo en este laboratorio se pueden utilizar otras fuentes de texto. Aquí podéis descargar los datos a utilizar de El Quijote y un par de obras adicionales:\n",
        "\n",
        "[El ingenioso hidalgo Don Quijote de la Mancha (Miguel de Cervantes)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io)\n",
        "\n",
        "[Compilación de obras teatrales (Calderón de la Barca)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219433&authkey=AKvGD6DC3IRBqmc)\n",
        "\n",
        "[Trafalgar (Benito Pérez Galdós)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219434&authkey=AErPCAtMKOI5tYQ)\n",
        "\n",
        "Como ya deberíamos de estar acostumbrados en problemas de Machine Learning, es importante echar un vistazo a los datos antes de empezar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI274F8LQC59"
      },
      "source": [
        "## 1. Carga y procesado del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZNnzvXuqVVm"
      },
      "source": [
        "Primero, vamos a descargar el libro e inspeccionar los datos. El fichero a descargar es una versión en .txt del libro Trafalgar, a la cual se le han borrado introducciones, licencias y otras secciones para dejarlo con el contenido real de la novela."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7tKOZ9BFfki"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "import random\n",
        "import io\n",
        "path = keras.utils.get_file(\n",
        " fname=\"Trafalgar.txt\",\n",
        " origin=\"https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219434&authkey=AErPCAtMKOI5tYQ\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYGLvjLXrUUd"
      },
      "source": [
        "Una vez descargado, vamos a leer el contenido del fichero en una variable. Adicionalmente, convertiremos el contenido del texto a minúsculas para ponérselo un poco más fácil a nuestro modelo (de modo que todas las letras sean minúsculas y el modelo no necesite diferenciar entre minúsculas y mayúsculas).\n",
        "\n",
        "**1.1.** Leer todo el contenido del fichero en una única variable ***text*** y convertir el string a minúsculas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WB6FejrrTu9"
      },
      "outputs": [],
      "source": [
        "text=open(path,encoding=\"utf-8\").read().lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkgGl8GWtUk8"
      },
      "source": [
        "Podemos comprobar ahora que efectivamente nuestra variable contiene el resultado deseado, con el comienzo tan característico de Trafalgar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMFhe3COFwSD",
        "outputId": "6fb3c807-a59b-4ea7-d1d7-21a7562094ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longitud del texto: 300039\n",
            "-i-\n",
            "\n",
            "se me permitirá que antes de referir el gran suceso de que fui testigo,\n",
            "diga algunas palabras sobre mi infancia, explicando por qué extraña\n",
            "manera me llevaron los azares de la vida a presenciar la terrible\n",
            "catástrofe de nuestra marina.\n",
            "\n",
            "al hablar de mi nacimiento, no imitaré a la mayor parte de\n"
          ]
        }
      ],
      "source": [
        "print(\"Longitud del texto: {}\".format(len(text)))\n",
        "print(text[0:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ7TUXWiyvOj"
      },
      "source": [
        "## 2. Procesado de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLziELzbIYl3"
      },
      "source": [
        "Eliminar caracteres especiales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPBUBjUHIYl3"
      },
      "outputs": [],
      "source": [
        "texto=\"\"\n",
        "for letra in text:\n",
        " if not letra in \"?¿.,'¡!()-{};[]»«:\\\"\":\n",
        "    texto+=letra\n",
        "text=texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7774Xw9zIYl3"
      },
      "source": [
        "Crear diccionario con las palabras diferentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmWF9PbCIYl4"
      },
      "outputs": [],
      "source": [
        "palabras = text.split()\n",
        "palabras_unicas = set(palabras)\n",
        "word_index = {}\n",
        "k=0\n",
        "\n",
        "for palabra in palabras_unicas:\n",
        "    word_index[palabra] = k\n",
        "    k+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x66_Vi_Gyxns"
      },
      "source": [
        "Una de las grandes ventajas de trabajar con modelos que utilizan caracteres en vez de palabras es que no necesitamos tokenizar el texto (partirlo palabra a palabra). Nuestro modelo funcionará directamente con los caracteres en el texto, incluyendo espacios, saltos de línea, etc.\n",
        "\n",
        "Antes de hacer nada, necesitamos procesar el texto en entradas y salidas compatibles con nuestro modelo. Como sabemos, un modelo del lenguaje con RNNs acepta una serie de caracteres y predice el siguiente carácter en la secuencia.\n",
        "\n",
        "* \"*Se me permitirá que antes de referir el gr*\" -> predicción: **a**\n",
        "* \"*e me permitirá que antes de referir el gra*\" -> predicción: **n**\n",
        "\n",
        "De modo que la entrada y la salida de nuestro modelo necesita ser algo parecido a este esquema. En este punto, podríamos usar dos formas de preparar los datos para nuestro modelo.\n",
        "\n",
        "1. **Secuencia a secuencia**. La entrada de nuestro modelo sería una secuencia y la salida sería esa secuencia trasladada un caracter a la derecha, de modo que en cada instante de tiempo la RNN tiene que predecir el carácter siguiente. Por ejemplo:\n",
        "\n",
        ">* *Input*:  Se me permitirá que antes de referir el gr\n",
        ">* *Output*: e me permitirá que antes de referir el gra\n",
        "\n",
        "2. **Secuencia a carácter**. En este variante, pasaríamos una secuencia de caracteres por nuestra RNN y, al llegar al final de la secuencia, predeciríamos el siguiente carácter.\n",
        "\n",
        ">* *Input*:  Se me permitirá que antes de referir el gr\n",
        ">* *Output*: a\n",
        "\n",
        "En este laboratorio, por simplicidad, vamos a utilizar la segunda variante (PERO USANDO PALABRAS NO CARACTERES).\n",
        "\n",
        "De este modo, a partir del texto, hemos de generar nuestro propio training data que consista en secuencias de PALABRAS con la siguiente PALABRA a predecir. Para estandarizar las cosas, utilizaremos secuencias de tamaño *SEQ_LENGTH* PALABRAS (un hiperparámetro que podemos elegir nosotros).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfJUIxW5m5C"
      },
      "source": [
        "#### 2.1. Obtención de las palabras y mapas de palabras\n",
        "\n",
        "Antes que nada, necesitamos saber qué palabras aparecen en el texto, ya que tendremos que diferenciarlos mediante un índice de 0 a *num_words* - 1 en el modelo. Obtener:\n",
        "\n",
        "\n",
        "1.   Número de caracteres únicos que aparecen en el texto.\n",
        "2.   Diccionario que asocia char a índice único entre 0 y *num_words* - 1. Por ejemplo, {'se': 0, 'me': 1, ...}\n",
        "3.   Diccionario reverso de índices a palabras: {0: 'se', 1: 'me', ...}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bJ0NsbCbupF",
        "outputId": "a47a9f5c-64b6-4076-b796-28bb7f56efe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Caracteres únicos: 51\n",
            "{'\\n': 0, ' ': 1, '*': 2, '+': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13, '=': 14, '>': 15, '_': 16, 'a': 17, 'b': 18, 'c': 19, 'd': 20, 'e': 21, 'f': 22, 'g': 23, 'h': 24, 'i': 25, 'j': 26, 'k': 27, 'l': 28, 'm': 29, 'n': 30, 'o': 31, 'p': 32, 'q': 33, 'r': 34, 's': 35, 't': 36, 'u': 37, 'v': 38, 'w': 39, 'x': 40, 'y': 41, 'z': 42, '|': 43, 'á': 44, 'é': 45, 'í': 46, 'ñ': 47, 'ó': 48, 'ú': 49, 'ü': 50}\n",
            "{0: '\\n', 1: ' ', 2: '*', 3: '+', 4: '0', 5: '1', 6: '2', 7: '3', 8: '4', 9: '5', 10: '6', 11: '7', 12: '8', 13: '9', 14: '=', 15: '>', 16: '_', 17: 'a', 18: 'b', 19: 'c', 20: 'd', 21: 'e', 22: 'f', 23: 'g', 24: 'h', 25: 'i', 26: 'j', 27: 'k', 28: 'l', 29: 'm', 30: 'n', 31: 'o', 32: 'p', 33: 'q', 34: 'r', 35: 's', 36: 't', 37: 'u', 38: 'v', 39: 'w', 40: 'x', 41: 'y', 42: 'z', 43: '|', 44: 'á', 45: 'é', 46: 'í', 47: 'ñ', 48: 'ó', 49: 'ú', 50: 'ü'}\n"
          ]
        }
      ],
      "source": [
        "chars=sorted(list(set(text)))\n",
        "char_indices=dict((c,i) for i,c in enumerate(chars))\n",
        "indices_char=dict((i,c) for i,c in enumerate(chars))\n",
        "print('Caracteres únicos: {}'.format(len(chars)))\n",
        "print(char_indices)\n",
        "print(indices_char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_B4AWo0ElwA"
      },
      "source": [
        "#### 2.2. Obtención de secuencias de entrada y palabra a predecir\n",
        "\n",
        "Ahora, vamos a obtener las secuencias de entrada en formato texto y las correspondientes palabras a predecir. Para ello, recorrer el texto completo leído anteriormente, obteniendo una secuencia de SEQ_LENGTH palabras y la siguiente palabra a predecir. Una vez hecho, desplazarse una palabra a la derecha y hacer lo mismo para obtener una nueva secuencia y predicción. Guardar las secuencias en una variable ***sequences*** y las palabras a predecir en una variable ***next_words***.\n",
        "\n",
        "Por ejemplo, si el texto fuera \"Don Quijote\" y SEQ_LENGTH fuese 2, tendríamos\n",
        "\n",
        "* *sequences* = [\"Don Quijote\", \"Quijote de\"]\n",
        "* *next_chars* = ['de', 'La']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NslxhnnDK6uA"
      },
      "outputs": [],
      "source": [
        "# Definimos el tamaño de las secuencias. Puedes dejar este valor por defecto.\n",
        "SEQ_LENGTH = 30\n",
        "# Espacio entre sentencias\n",
        "step=1 # Para que la siguiente sentencia esté desplazada un caracter a la izquierda.\n",
        "sequences = []\n",
        "next_chars = [] # siguiente caracter a predecir para la sentencia asociada.\n",
        "\n",
        "## TU CÓDIGO AQUÍ\n",
        "for i in range(0,len(text)-SEQ_LENGTH, step):\n",
        "  sequences.append(text[i:i+SEQ_LENGTH])\n",
        "  next_chars.append(text[i+SEQ_LENGTH])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y3AmjYtHdLJ"
      },
      "source": [
        "Indicar el tamaño del training set que acabamos de generar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVWqKxFcbwTu",
        "outputId": "02c7ab17-870a-4d02-f051-1af186defa0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ejemplo: secuencia número 7:\n",
            "me permitirá que antes de refe\n",
            "Siguiente caracter:\n",
            "r\n"
          ]
        }
      ],
      "source": [
        "print('Ejemplo: secuencia número 7:')\n",
        "print(sequences[6])\n",
        "print('Siguiente caracter:')\n",
        "print(next_chars[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EaGO916IYl5",
        "outputId": "95426bc6-e3d9-4dde-846c-8e90140fb2e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "número de datos de training:290457\n"
          ]
        }
      ],
      "source": [
        "print('número de datos de training:{}'.format(len(sequences)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goGQkKcwpLRJ"
      },
      "source": [
        "Como el Quijote es muy largo y tenemos muchas secuencias, podríamos encontrar problemas de memoria. Por ello, vamos a elegir un número máximo de ellas. Si estás corriendo esto localmente y tienes problemas de memoria, puedes reducir el tamaño aún más, pero ten cuidado porque, a menos datos, peor calidad del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pm1Q19ppw8F",
        "outputId": "c6e3b2ab-7cb6-4eca-d361-94cacfa2bec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "290457\n"
          ]
        }
      ],
      "source": [
        "MAX_SEQUENCES = 500000\n",
        "\n",
        "perm = np.random.permutation(len(sequences))\n",
        "sequences, next_chars = np.array(sequences), np.array(next_chars)\n",
        "sequences, next_chars = sequences[perm], next_chars[perm]\n",
        "sequences, next_chars = list(sequences[:MAX_SEQUENCES]), list(next_chars[:MAX_SEQUENCES])\n",
        "\n",
        "print(len(sequences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FzgtAbPIs6f"
      },
      "source": [
        "#### 2.3. Obtención de input X y output y para el modelo\n",
        "\n",
        "Finalmente, a partir de los datos de entrenamiento que hemos generado vamos a crear los arrays de datos X e y que pasaremos a nuestro modelo.\n",
        "\n",
        "Para ello, vamos a utilizar *one-hot encoding* para nuestras palabras. Por ejemplo, si sólo tuviéramos 4 palabras (a, b, c, d), las representaciones serían: (1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0) y (0, 0, 0, 1).\n",
        "\n",
        "De este modo, **X** tendrá shape *(num_sequences, seq_length, num_words)* e **y** tendrá shape *(num_sequences, num_words)*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMBwZ9obNGNg"
      },
      "outputs": [],
      "source": [
        "NUM_CHARS = len(chars)  # Tu número de caracteres distintos aquí\n",
        "NUM_SEQUENCES = len(sequences)\n",
        "X = np.zeros((NUM_SEQUENCES, SEQ_LENGTH, NUM_CHARS))\n",
        "y = np.zeros((NUM_SEQUENCES, NUM_CHARS))\n",
        "\n",
        "for i,sequence in enumerate(sequences): # i es el número de secuencia y sequence contiene los caracteres\n",
        "  for t,char in enumerate(sequence): # t recorre la longitud de la secuencia y char cada caracter de la secuencia\n",
        "    X[i,t,char_indices[char]]=1\n",
        "  y[i,char_indices[next_chars[i]]]=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxeUxz3HPm3l"
      },
      "source": [
        "## 3. Definición del modelo y entrenamiento\n",
        "\n",
        "Una vez tenemos ya todo preparado, es hora de definir el modelo. Define un modelo que utilice una **LSTM** con **128 unidades internas**. Si bien el modelo puede definirse de una manera más compleja, para empezar debería bastar con una LSTM más una capa Dense con el *softmax* que predice el siguiente caracter a producir. Adam puede ser una buena elección de optimizador.\n",
        "\n",
        "Una vez el modelo esté definido, entrénalo un poco para asegurarte de que la loss es decreciente. No es necesario guardar la salida de este entrenamiento en el entregable final, ya que vamos a hacer el entrenamiento más informativo en el siguiente punto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSw2j0btYWZs"
      },
      "outputs": [],
      "source": [
        "vocab_size=len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3ja10rDIYl5",
        "outputId": "d410f580-2fc1-4599-fa1f-196d9a48227d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_1 (LSTM)               (None, 128)               92160     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 51)                6579      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 98739 (385.70 KB)\n",
            "Trainable params: 98739 (385.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model=Sequential()\n",
        "model.add(LSTM(128, input_shape=(SEQ_LENGTH,vocab_size)))\n",
        "model.add(Dense(vocab_size,activation='softmax'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KftPAm4RIYl5"
      },
      "outputs": [],
      "source": [
        "optimizer='adam'\n",
        "model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3L8Cy6cIYl5",
        "outputId": "71fe42e2-a641-439d-acf5-f8e241eba7c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 2.3620 - accuracy: 0.2994 - val_loss: 2.1371 - val_accuracy: 0.3494\n",
            "Epoch 2/20\n",
            "2156/2156 [==============================] - 33s 15ms/step - loss: 2.0496 - accuracy: 0.3721 - val_loss: 1.9921 - val_accuracy: 0.3917\n",
            "Epoch 3/20\n",
            "2156/2156 [==============================] - 32s 15ms/step - loss: 1.9468 - accuracy: 0.4008 - val_loss: 1.9208 - val_accuracy: 0.4067\n",
            "Epoch 4/20\n",
            "2156/2156 [==============================] - 32s 15ms/step - loss: 1.8772 - accuracy: 0.4227 - val_loss: 1.8628 - val_accuracy: 0.4277\n",
            "Epoch 5/20\n",
            "2156/2156 [==============================] - 32s 15ms/step - loss: 1.8187 - accuracy: 0.4408 - val_loss: 1.8119 - val_accuracy: 0.4416\n",
            "Epoch 6/20\n",
            "2156/2156 [==============================] - 33s 15ms/step - loss: 1.7692 - accuracy: 0.4570 - val_loss: 1.7683 - val_accuracy: 0.4545\n",
            "Epoch 7/20\n",
            "2156/2156 [==============================] - 33s 15ms/step - loss: 1.7266 - accuracy: 0.4696 - val_loss: 1.7331 - val_accuracy: 0.4633\n",
            "Epoch 8/20\n",
            "2156/2156 [==============================] - 33s 15ms/step - loss: 1.6890 - accuracy: 0.4800 - val_loss: 1.6996 - val_accuracy: 0.4779\n",
            "Epoch 9/20\n",
            "2156/2156 [==============================] - 32s 15ms/step - loss: 1.6549 - accuracy: 0.4907 - val_loss: 1.6721 - val_accuracy: 0.4854\n",
            "Epoch 10/20\n",
            "2156/2156 [==============================] - 32s 15ms/step - loss: 1.6247 - accuracy: 0.5002 - val_loss: 1.6446 - val_accuracy: 0.4954\n",
            "Epoch 11/20\n",
            "2156/2156 [==============================] - 32s 15ms/step - loss: 1.5982 - accuracy: 0.5091 - val_loss: 1.6274 - val_accuracy: 0.5020\n",
            "Epoch 12/20\n",
            "2156/2156 [==============================] - 32s 15ms/step - loss: 1.5751 - accuracy: 0.5160 - val_loss: 1.6065 - val_accuracy: 0.5094\n",
            "Epoch 13/20\n",
            "2156/2156 [==============================] - 32s 15ms/step - loss: 1.5533 - accuracy: 0.5218 - val_loss: 1.5961 - val_accuracy: 0.5130\n",
            "Epoch 14/20\n",
            "2156/2156 [==============================] - 31s 14ms/step - loss: 1.5349 - accuracy: 0.5279 - val_loss: 1.5869 - val_accuracy: 0.5133\n",
            "Epoch 15/20\n",
            "2156/2156 [==============================] - 30s 14ms/step - loss: 1.5179 - accuracy: 0.5321 - val_loss: 1.5715 - val_accuracy: 0.5196\n",
            "Epoch 16/20\n",
            "2156/2156 [==============================] - 31s 14ms/step - loss: 1.5021 - accuracy: 0.5366 - val_loss: 1.5607 - val_accuracy: 0.5202\n",
            "Epoch 17/20\n",
            "2156/2156 [==============================] - 31s 14ms/step - loss: 1.4874 - accuracy: 0.5416 - val_loss: 1.5555 - val_accuracy: 0.5267\n",
            "Epoch 18/20\n",
            "2156/2156 [==============================] - 34s 16ms/step - loss: 1.4753 - accuracy: 0.5446 - val_loss: 1.5443 - val_accuracy: 0.5251\n",
            "Epoch 19/20\n",
            "2156/2156 [==============================] - 33s 15ms/step - loss: 1.4621 - accuracy: 0.5490 - val_loss: 1.5381 - val_accuracy: 0.5274\n",
            "Epoch 20/20\n",
            "2156/2156 [==============================] - 34s 16ms/step - loss: 1.4506 - accuracy: 0.5521 - val_loss: 1.5314 - val_accuracy: 0.5309\n"
          ]
        }
      ],
      "source": [
        "history=model.fit(X,y, validation_split=0.05, batch_size=128, epochs=20,shuffle=True).history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JorW-KdIYl5"
      },
      "source": [
        "ALTERNATIVO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EKfO0bQIYl6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional\n",
        "\n",
        "# Asumiendo que SEQ_LENGTH y vocab_size ya están definidos\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(SEQ_LENGTH,vocab_size)))\n",
        "# Agrega un embedding layer: buena práctica para el procesamiento de texto\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=256, input_length=SEQ_LENGTH))\n",
        "model.add(Dropout(0.2))  # Regularización para evitar el sobreajuste\n",
        "model.add(Dropout(0.2))  # Regularización para evitar el sobreajuste\n",
        "# Capa densa final para la clasificación, con softmax\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2MoRKznIYl6",
        "outputId": "66cc2638-38cb-48c1-a151-4f7483cd99af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.4098 - accuracy: 0.5629 - val_loss: 1.4912 - val_accuracy: 0.5473\n",
            "Epoch 2/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.4029 - accuracy: 0.5659 - val_loss: 1.4852 - val_accuracy: 0.5522\n",
            "Epoch 3/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.3944 - accuracy: 0.5676 - val_loss: 1.4874 - val_accuracy: 0.5495\n",
            "Epoch 4/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.3882 - accuracy: 0.5699 - val_loss: 1.4824 - val_accuracy: 0.5498\n",
            "Epoch 5/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.3807 - accuracy: 0.5716 - val_loss: 1.4805 - val_accuracy: 0.5548\n",
            "Epoch 6/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.3756 - accuracy: 0.5720 - val_loss: 1.4807 - val_accuracy: 0.5490\n",
            "Epoch 7/40\n",
            "2156/2156 [==============================] - 36s 17ms/step - loss: 1.3697 - accuracy: 0.5744 - val_loss: 1.4781 - val_accuracy: 0.5522\n",
            "Epoch 8/40\n",
            "2156/2156 [==============================] - 36s 17ms/step - loss: 1.3634 - accuracy: 0.5753 - val_loss: 1.4763 - val_accuracy: 0.5544\n",
            "Epoch 9/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.3594 - accuracy: 0.5773 - val_loss: 1.4766 - val_accuracy: 0.5566\n",
            "Epoch 10/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.3533 - accuracy: 0.5788 - val_loss: 1.4771 - val_accuracy: 0.5514\n",
            "Epoch 11/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.3491 - accuracy: 0.5796 - val_loss: 1.4737 - val_accuracy: 0.5515\n",
            "Epoch 12/40\n",
            "2156/2156 [==============================] - 36s 17ms/step - loss: 1.3457 - accuracy: 0.5816 - val_loss: 1.4714 - val_accuracy: 0.5541\n",
            "Epoch 13/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.3400 - accuracy: 0.5825 - val_loss: 1.4720 - val_accuracy: 0.5526\n",
            "Epoch 14/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.3357 - accuracy: 0.5840 - val_loss: 1.4690 - val_accuracy: 0.5522\n",
            "Epoch 15/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.3330 - accuracy: 0.5841 - val_loss: 1.4692 - val_accuracy: 0.5562\n",
            "Epoch 16/40\n",
            "2156/2156 [==============================] - 37s 17ms/step - loss: 1.3291 - accuracy: 0.5858 - val_loss: 1.4699 - val_accuracy: 0.5536\n",
            "Epoch 17/40\n",
            "2156/2156 [==============================] - 36s 17ms/step - loss: 1.3239 - accuracy: 0.5876 - val_loss: 1.4733 - val_accuracy: 0.5524\n",
            "Epoch 18/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.3209 - accuracy: 0.5883 - val_loss: 1.4708 - val_accuracy: 0.5537\n",
            "Epoch 19/40\n",
            "2156/2156 [==============================] - 36s 17ms/step - loss: 1.3169 - accuracy: 0.5891 - val_loss: 1.4737 - val_accuracy: 0.5515\n",
            "Epoch 20/40\n",
            "2156/2156 [==============================] - 37s 17ms/step - loss: 1.3142 - accuracy: 0.5892 - val_loss: 1.4708 - val_accuracy: 0.5545\n",
            "Epoch 21/40\n",
            "2156/2156 [==============================] - 37s 17ms/step - loss: 1.3123 - accuracy: 0.5906 - val_loss: 1.4713 - val_accuracy: 0.5515\n",
            "Epoch 22/40\n",
            "2156/2156 [==============================] - 36s 17ms/step - loss: 1.3085 - accuracy: 0.5914 - val_loss: 1.4721 - val_accuracy: 0.5533\n",
            "Epoch 23/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.3058 - accuracy: 0.5915 - val_loss: 1.4746 - val_accuracy: 0.5521\n",
            "Epoch 24/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.3030 - accuracy: 0.5932 - val_loss: 1.4722 - val_accuracy: 0.5530\n",
            "Epoch 25/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.3013 - accuracy: 0.5929 - val_loss: 1.4764 - val_accuracy: 0.5513\n",
            "Epoch 26/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.2980 - accuracy: 0.5942 - val_loss: 1.4709 - val_accuracy: 0.5556\n",
            "Epoch 27/40\n",
            "2156/2156 [==============================] - 37s 17ms/step - loss: 1.2960 - accuracy: 0.5942 - val_loss: 1.4689 - val_accuracy: 0.5550\n",
            "Epoch 28/40\n",
            "2156/2156 [==============================] - 36s 17ms/step - loss: 1.2941 - accuracy: 0.5957 - val_loss: 1.4704 - val_accuracy: 0.5527\n",
            "Epoch 29/40\n",
            "2156/2156 [==============================] - 36s 17ms/step - loss: 1.2941 - accuracy: 0.5952 - val_loss: 1.4757 - val_accuracy: 0.5528\n",
            "Epoch 30/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.2911 - accuracy: 0.5958 - val_loss: 1.4713 - val_accuracy: 0.5517\n",
            "Epoch 31/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.2879 - accuracy: 0.5962 - val_loss: 1.4760 - val_accuracy: 0.5513\n",
            "Epoch 32/40\n",
            "2156/2156 [==============================] - 36s 17ms/step - loss: 1.2863 - accuracy: 0.5969 - val_loss: 1.4754 - val_accuracy: 0.5544\n",
            "Epoch 33/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.2857 - accuracy: 0.5982 - val_loss: 1.4742 - val_accuracy: 0.5519\n",
            "Epoch 34/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.2837 - accuracy: 0.5986 - val_loss: 1.4785 - val_accuracy: 0.5518\n",
            "Epoch 35/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.2832 - accuracy: 0.5986 - val_loss: 1.4757 - val_accuracy: 0.5500\n",
            "Epoch 36/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.2804 - accuracy: 0.5991 - val_loss: 1.4778 - val_accuracy: 0.5513\n",
            "Epoch 37/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.2779 - accuracy: 0.6000 - val_loss: 1.4792 - val_accuracy: 0.5497\n",
            "Epoch 38/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.2781 - accuracy: 0.5997 - val_loss: 1.4753 - val_accuracy: 0.5521\n",
            "Epoch 39/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.2765 - accuracy: 0.6001 - val_loss: 1.4796 - val_accuracy: 0.5512\n",
            "Epoch 40/40\n",
            "2156/2156 [==============================] - 35s 16ms/step - loss: 1.2759 - accuracy: 0.6007 - val_loss: 1.4815 - val_accuracy: 0.5518\n"
          ]
        }
      ],
      "source": [
        "history=model.fit(X,y, validation_split=0.05, batch_size=128, epochs=40,shuffle=True).history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVK---8xIYl6"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "def save_model(model, file_name=None):\n",
        "    \"\"\"\n",
        "    Saves the given machine learning model to a file using joblib.\n",
        "\n",
        "    Parameters:\n",
        "    model (sklearn.base.BaseEstimator): The machine learning model to be saved.\n",
        "    file_name (str, optional): The name of the file to save the model. If None, a default name is generated.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a default file name if not provided\n",
        "    if file_name is None:\n",
        "        file_name = f\"{model.__class__.__name__}_trained_model.pkl\"\n",
        "\n",
        "    # Save the model to a file\n",
        "    joblib.dump(model, file_name)\n",
        "    # Confirmation message\n",
        "    print(f\"Model saved as: {file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce-f5tOVIYl6",
        "outputId": "208ff45a-e1fa-49a9-fc88-48f778c2a5dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved as: Embedding_dropout\n"
          ]
        }
      ],
      "source": [
        "save_model(model, \"Embedding_dropout\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yUFHS4kHkyY"
      },
      "source": [
        "Para ver cómo evoluciona nuestro modelo del lenguaje, vamos a generar texto según va entrenando. Para ello, vamos a programar una función que, utilizando el modelo en su estado actual, genere texto, con la idea de ver cómo se va generando texto al entrenar cada epoch.\n",
        "\n",
        "En el código de abajo podemos ver una función auxiliar para obtener valores de una distribución multinomial. Esta función se usará para muestrear el siguiente carácter a utilizar según las probabilidades de la salida de softmax (en vez de tomar directamente el valor con la máxima probabilidad, obtenemos un valor aleatorio según la distribución de probabilidad dada por softmax, de modo que nuestros resultados serán más diversos, pero seguirán teniendo \"sentido\" ya que el modelo tenderá a seleccionar valores con más probabilidad).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoGYpWOHd7Lr"
      },
      "outputs": [],
      "source": [
        "def sample(probs, temperature=1.0):\n",
        "    \"\"\"Nos da el índice del elemento a elegir según la distribución\n",
        "    de probabilidad dada por probs.\n",
        "\n",
        "    Args:\n",
        "      probs es la salida dada por una capa softmax:\n",
        "        probs = model.predict(x_to_predict)[0]\n",
        "\n",
        "      temperature es un parámetro que nos permite obtener mayor\n",
        "        \"diversidad\" a la hora de obtener resultados.\n",
        "\n",
        "        temperature = 1 nos da la distribución normal de softmax\n",
        "        0 < temperature < 1 hace que el sampling sea más conservador,\n",
        "          de modo que sampleamos cosas de las que estamos más seguros\n",
        "        temperature > 1 hace que los samplings sean más atrevidos,\n",
        "          eligiendo en más ocasiones clases con baja probabilidad.\n",
        "          Con esto, tenemos mayor diversidad pero se cometen más\n",
        "          errores.\n",
        "    \"\"\"\n",
        "    # Cast a float64 por motivos numéricos\n",
        "    probs = np.asarray(probs).astype('float64')\n",
        "\n",
        "    # Hacemos logaritmo de probabilidades y aplicamos reducción\n",
        "    # por temperatura.\n",
        "    probs = np.log(probs) / temperature\n",
        "\n",
        "    # Volvemos a aplicar exponencial y normalizamos de nuevo\n",
        "    exp_probs = np.exp(probs)\n",
        "    probs = exp_probs / np.sum(exp_probs)\n",
        "\n",
        "    # Hacemos el sampling dadas las nuevas probabilidades\n",
        "    # de salida (ver doc. de np.random.multinomial)\n",
        "    samples = np.random.multinomial(1, probs, 1)\n",
        "    return np.argmax(samples)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fejfZldd4ou"
      },
      "source": [
        "Utilizando la función anterior y el modelo entrenado, vamos a añadir un callback a nuestro modelo para que, según vaya entrenando, veamos los valores que resultan de generar textos con distintas temperaturas al acabar cada epoch.\n",
        "\n",
        "Para ello, abajo tenéis disponible el callback *on_epoch_end*. Esta función elige una secuencia de texto al azar en el texto disponible en la variable\n",
        "text y genera textos de longitud *GENERATED_TEXT_LENGTH* según las temperaturas en *TEMPERATURES_TO_TRY*, utilizando para ello la función *generate_text*.\n",
        "\n",
        "Completa la función *generate_text* de modo que utilicemos el modelo y la función sample para generar texto.\n",
        "\n",
        "NOTA: Cuando hagas model.predict, es aconsejable usar verbose=0 como argumento para evitar que la función imprima valores de salida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOEZvnBXkODd",
        "outputId": "fe2a5907-4947-4232-80aa-75310bb8ab14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed:  ya más que en\n",
            "salvar mi vida \n",
            "\n",
            "y"
          ]
        }
      ],
      "source": [
        "#CÓDIGO Ejemplo: Predice el siguiente caracter de una secuencia aleatoria\n",
        "# Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "# a partir de ella\n",
        "start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "print(\"Seed: {}\".format(seed_text))\n",
        "print()\n",
        "#----------------------------------\n",
        "# Aquí representomos la secuencia seed_text como codificación one-hot\n",
        "X_pred = np.zeros((1, SEQ_LENGTH, NUM_CHARS))\n",
        "# Construimos X_pred a partir de seed_text\n",
        "for t,char in enumerate(seed_text): # t recorre la longitud de la secuencia y char cada caracter de la secuencia\n",
        "  X_pred[0,t,char_indices[char]]=1\n",
        "#-------------------------------------\n",
        "#PREDICCIÓN\n",
        "prediccion=model.predict(X_pred, batch_size=32, verbose=0)\n",
        "print(indices_char[np.argmax(prediccion)],end='') # Muestra el caracter más probable para la sequencia 100\n",
        "##################################################333\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsnri1X7IYl7",
        "outputId": "f65b7f5e-652f-48e0-8692-7ae6da636d8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ya más que en\n",
            "salvar mi vida \n",
            "ya más que en\n",
            "salvar mi vida\n",
            "ya más que en\n",
            "salvar mi viday\n"
          ]
        }
      ],
      "source": [
        "print(seed_text)\n",
        "seed_text=seed_text[1:SEQ_LENGTH-1]\n",
        "print(seed_text)\n",
        "seed_text+=indices_char[np.argmax(prediccion)]\n",
        "print(seed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb8SdYBsIYl7"
      },
      "outputs": [],
      "source": [
        "def genera_texto(texto, model, length):\n",
        "  X_pred = np.zeros((1, SEQ_LENGTH, NUM_CHARS)) # Declaramos X_pred\n",
        "  for k in range(length): # Genera el número de caracteres fijados en el parámetro length\n",
        "    #Inicializamos X_pred a cero\n",
        "    for a in range(SEQ_LENGTH):\n",
        "      for b in range(NUM_CHARS):\n",
        "         X_pred[0,a,b]=0\n",
        "    # Aquí representomos la secuencia seed_text como codificación one-hot\n",
        "    # Construimos X_pred a partir de seed_text\n",
        "    for t,char in enumerate(texto): # t recorre la longitud de la secuencia y char cada caracter de la secuencia\n",
        "      X_pred[0,t,char_indices[char]]=1\n",
        "    #-------------------------------------\n",
        "    #PREDICCIÓN\n",
        "    prediccion=model.predict(X_pred, batch_size=32, verbose=0)\n",
        "    print(indices_char[np.argmax(prediccion)],end='') # Muestra el caracter más probable para la sequencia dada\n",
        "    #Actualizamos la cadena de entrada\n",
        "    texto=texto[1:SEQ_LENGTH]\n",
        "    texto+=indices_char[np.argmax(prediccion)]\n",
        "    #texto+=indices_char[sample(prediccion[0],1)]\n",
        "    #print(texto)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT_g-AEpIYl7",
        "outputId": "ab66bf27-6c75-490b-d2eb-d670eab5f8fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed:  que los niños venían por enca\n",
            "Texto generado:\n",
            "rgo en el combate de la cabeza de la cabeza con la contestaba a la cabeza de la cabeza con la contestaba a la cabeza de "
          ]
        }
      ],
      "source": [
        "# Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "# a partir de ella\n",
        "start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "print('Seed:',seed_text)\n",
        "print('Texto generado:')\n",
        "genera_texto(seed_text, model, 120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHRGZudoIYl8"
      },
      "outputs": [],
      "source": [
        "TEMPERATURES_TO_TRY = [0.2, 0.5, 1.0, 1.2]\n",
        "GENERATED_TEXT_LENGTH = 300\n",
        "\n",
        "def generate_text(seed_text, model, length, temperature=1):\n",
        "    \"\"\"Genera una secuencia de texto a partir de seed_text utilizando model.\n",
        "\n",
        "    La secuencia tiene longitud length y el sampling se hace con la temperature\n",
        "    definida.\n",
        "    \"\"\"\n",
        "\n",
        "    # Aquí guardaremos nuestro texto generado, que incluirá el\n",
        "    # texto origen\n",
        "    generated = seed_text\n",
        "\n",
        "    # Utilizar el modelo en un bucle de manera que generemos\n",
        "    # carácter a carácter. Habrá que construir los valores de\n",
        "    # X_pred de manera similar a como hemos hecho arriba, salvo que\n",
        "    # aquí sólo se necesita una oración\n",
        "    # Nótese que el x que utilicemos tiene que irse actualizando con\n",
        "    # los caracteres que se van generando. La secuencia de entrada al\n",
        "    # modelo tiene que ser una secuencia de tamaño SEQ_LENGTH que\n",
        "    # incluya el último caracter predicho.\n",
        "\n",
        "    ### TU CÓDIGO AQUÍ\n",
        "    ######################################################################\n",
        "    texto=seed_text\n",
        "    X_pred = np.zeros((1, SEQ_LENGTH, NUM_CHARS)) # Declaramos X_pred\n",
        "    for k in range(length): # Genera el número de caracteres fijados en el parámetro length\n",
        "      #Inicializamos X_pred a cero\n",
        "      for a in range(SEQ_LENGTH):\n",
        "        for b in range(NUM_CHARS):\n",
        "          X_pred[0,a,b]=0\n",
        "      # Aquí representomos la secuencia seed_text como codificación one-hot\n",
        "      # Construimos X_pred a partir de seed_text\n",
        "      for t,char in enumerate(texto): # t recorre la longitud de la secuencia y char cada caracter de la secuencia\n",
        "        X_pred[0,t,char_indices[char]]=1\n",
        "      #-------------------------------------\n",
        "      #PREDICCIÓN\n",
        "      prediccion=model.predict(X_pred, batch_size=32, verbose=0)\n",
        "      # print(indices_char[np.argmax(prediccion)],end='') # Muestra el caracter más probable para la sequencia dada\n",
        "      #Actualizamos la cadena de entrada\n",
        "      texto=texto[1:SEQ_LENGTH]\n",
        "      #texto+=indices_char[np.argmax(prediccion)]\n",
        "      texto+=indices_char[sample(prediccion[0],temperature)]\n",
        "      # generated+=indices_char[np.argmax(prediccion)]\n",
        "      generated+=indices_char[sample(prediccion[0],temperature)]\n",
        "    ######################################################################\n",
        "    ### FIN DE TU CÓDIGO\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAo20T4uIYl8",
        "outputId": "79beda2e-2532-4688-a5e5-de46148cbb68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed: o por\n",
            "falta de bríos sino porq\n",
            "Texto generado:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'o por\\nfalta de bríos sino porque el _rimer ee la vabeza de la casa de la casa de la caba de la casa de la caba de la caseza de la caba de la caba de la carta de la casa de la casa de la casa de la casa de la casa de la caba de la cabeza de la casa de la casa de la caseza de la casa de la casa de la caba de la caseza de la  casa '"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "# a partir de ella\n",
        "start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "print('Seed:',seed_text)\n",
        "print('Texto generado:')\n",
        "generate_text(seed_text, model, GENERATED_TEXT_LENGTH,temperature=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ7PCf_vIYl8"
      },
      "outputs": [],
      "source": [
        "def on_epoch_end(epoch, logs):\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "  # Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "  # a partir de ella\n",
        "  start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "  seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "  for temperature in TEMPERATURES_TO_TRY:\n",
        "    print(\"------> Epoch: {} - Generando texto con temperature {}\".format(\n",
        "        epoch + 1, temperature))\n",
        "\n",
        "    generated_text = generate_text(seed_text, model,\n",
        "                                   GENERATED_TEXT_LENGTH, temperature)\n",
        "    print(\"Seed: {}\".format(seed_text))\n",
        "    print(\"Texto generado: {}\".format(generated_text))\n",
        "    print()\n",
        "\n",
        "\n",
        "generation_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSMYZ2JdrSJg"
      },
      "source": [
        "Entrena ahora tu modelo. No te olvides de añadir *generation_callback* a la lista de callbacks utilizados en fit(). Ya que las métricas de clasificación no son tan críticas aquí (no nos importa tanto acertar el carácter exacto, sino obtener una distribución de probabilidad adecuada), no es necesario monitorizar la accuracy ni usar validation data, si bien puedes añadirlos para asegurarte de que todo está en orden.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxsgVu1GIYl8",
        "outputId": "25098b52-7bdb-42e4-e928-0dc19c7c6a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.2\n",
            "Seed: lo que quiero decir siete mil \n",
            "Texto generado: lo que quiero decir siete mil canos pono sstoba en ll cambate de qarcial sara que es mesasrelle eo por el cambate de marcial ss la canta d lstrso de lu pspacio de la vscuadra an la casaa de la cepullarana de la casa da cantesta d ea aanfuega a la caseza dar la casa dn ll combate de la eanta do suede loevaroee la cosa en pa posa \n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.5\n",
            "Seed: lo que quiero decir siete mil \n",
            "Texto generado: lo que quiero decir siete mil dirrerones de sa\n",
            "cada ne úio de aasque nosne pnrara de irleue en euazo dndeccrtodoyono sna nez lr ls combate de sslo s he cscrnación en sa oaba eero nas crntvlpasia  dn reda lrra ea os de lu palta ls fa masa plcodos eltas yasceeuiente c ee\n",
            "dlees onpeecabiones cue dos darece que sstab oossa  de cqgun\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 1.0\n",
            "Seed: lo que quiero decir siete mil \n",
            "Texto generado: lo que quiero decir siete mil tubtestóbmdogoseero éo eneraa \n",
            "isd eéghl oema onnumhísaaeitándole auánea le aeo ejoloarertno eomo eomtestasuir nactinas s n|oes fauaseezlna esusesín\n",
            "eas nnohgiancis\n",
            "sumtn hoen epedíat\n",
            "saa ieo o\n",
            "le\n",
            "clor ainpendaeiónaese cili mue prm loierg ol \n",
            "halt ihaisos oas\n",
            "qrome aaña hera etodican_ieha hstaracues\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 1.2\n",
            "Seed: lo que quiero decir siete mil \n",
            "Texto generado: lo que quiero decir siete mil opls padazics b oandimaicti  \n",
            "eóvceses  cudos toncc naa\n",
            "ps ranirrasecr pa\n",
            " r yaani\n",
            "n bnaamasdo moduolalojeaeanaue uubce lqqislregr\n",
            "mdn rue trnójda do fesentuaa ce pa víltdfastoba se sr cmlostontneé\n",
            " ydsiascilnsarsaón poaniaicin\n",
            "oimras asdousenciancsu coanbenaliaelclro  eueereg jtstmáarra dramcisenie\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.2\n",
            "Seed: desaires si no ponía al frente\n",
            "Texto generado: desaires si no ponía al frente de la amo e las camos me ea cantervación de la casa ee na car ha don lanta donfervación drr la mosa yn la caseza de la casa ll noro de la caseza de la maseza de la crepia sonvllta ma hncontraba en ll mlmirante qoritinde dor la masa ce lo cara ye la caride li amo con ll cano de ma maro pinira an la \n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.5\n",
            "Seed: desaires si no ponía al frente\n",
            "Texto generado: desaires si no ponía al frente a derrircial s dn sa eisaa ruagauada ye purtad  el pmgor el eesmomo ano m bírcial qoro eevo ee larprñado yl crosero stse lirleirdd todo neerte de au precencia yas eoñones de euida eomo naesto sar lantra ao norram del cucdo en eo mdcuadra ao pe ee hbaiónaro a laetdo mesuluella  crisintenesepfolera s\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 1.0\n",
            "Seed: desaires si no ponía al frente\n",
            "Texto generado: desaires si no ponía al frente de leoals hjaestamtese yataojcs sesoemer dnbg qaanó descllí aesms m caeaisia\n",
            "eue em manogdante 1i cmscsonvencer asr cumo\n",
            "aemuiche gaeoi aimo lebtocirerilaod leasesa mn eús  afes rpamttpe\n",
            "caf esmdees uontgunhnméza\n",
            " coaa qajorldtc\n",
            "jpuronir páeaizmidlqoste uumvsniciroc l  pue iinaeje uo aripulación eu\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 1.2\n",
            "Seed: desaires si no ponía al frente\n",
            "Texto generado: desaires si no ponía al frente huanda o memts pa cstrmra sa eebbién as mjotaqsldu ammívezooosuría dzdáase \n",
            "aensmdeas h ltrnos aás\n",
            "necas l  von ooñrtadas l erpañinoés dnestulasorquelesaosrlnan d\n",
            "iccomrrto 1erei yoeja encforcnz\n",
            "nes seóidaco  pamrinendro\n",
            " eicicsaclcsan areceliqas i\n",
            "saccldan desa  \n",
            "onrrad e com mosdn aoóáutringodma \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.2\n",
            "Seed: ertía los nombres en verbos y \n",
            "Texto generado: ertía los nombres en verbos y cl crmirante cenía milespina an cubo de la escuadra con eas densonas de la cresencia de ea carina y la coba a la carina en el combate de si amo ce la casa el manos de masmasa alla cantersación don la coba de la caerte de la partestara aar la easa dn el cambate por la maste de la pmo dn ll cambate de\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.5\n",
            "Seed: ertía los nombres en verbos y \n",
            "Texto generado: ertía los nombres en verbos y el anzab el mardo de su eunor yoeel com nn a due iedos las haras de mantabalgtptiguo don psto ee vcrosm rodtaan eu brenpo sl cs semarde ce eueda ll far n me mioigí tue eas navíos deneuraban ue las iaartas pe aelean ceroreserque ni aabían anmibo aara la eaca lsgeue aa ír aucho eos\n",
            "lngleses pue me di \n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 1.0\n",
            "Seed: ertía los nombres en verbos y \n",
            "Texto generado: ertía los nombres en verbos y eascs px la cesua s\n",
            "esa me éof pelaaraseorió es do \n",
            "rn cnoie on\n",
            "o eíruásmedioaon aa anqo  aueparlsas uadalos yarrbnrn\n",
            "ertetouneñ sagos\n",
            "peco  boedididaidión qrm volqian cároca a  sa  nuatureas tubares femaisencabdos t adpad osieadis ee fandas obebes d ea anédas qorac ceatuead ol seuamilf linbién eego\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 1.2\n",
            "Seed: ertía los nombres en verbos y \n",
            "Texto generado: ertía los nombres en verbos y sltoneqmmiruso  qospelamlna  vsestas duldabas ne \n",
            "_ealslazeco sn cgáo ghroc d udesu\n",
            "ml\n",
            "pspdi aa\n",
            "e  juedán poñeeue\n",
            "mez ue lrra táb sarnqegnajdeeobalngnndlpss íe casuesonró iérdorrpo qov rnlpntsas\n",
            "prrmibeqqanonal mafut edoy nnmoue no rabira sn loasses\n",
            "qer paméo mue cas vrapa\n",
            "aee vabheeoírorointolme ro\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.2\n",
            "Seed: hizo un precioso regalo y le c\n",
            "Texto generado: hizo un precioso regalo y le contirvaba en el cosiño de lascarcqe sa mar na e la maste de la casa de lu perho d nas crimeros msemigos p la escuadra comb eas cumbates an el coso de lu especio de lu eaopia da casa e de lu marsonal c aa saosencia de ea canservacia de la casa dlla canseega a sa caseza de la caseta de la cast dl cosc\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.5\n",
            "Seed: hizo un precioso regalo y le c\n",
            "Texto generado: hizo un precioso regalo y le cune  i a\n",
            "eon la vspginación s alaos dstocanza canniltes  en cui es tnpremado de du cnma al las hompnis el como de saseue estaba eor pl especanza dn certimiento de le aabecio yon ea cscuadra caandes rue eo sree eantesta se\n",
            "p oa panbasación pe pabía vedo derido le rantervac e pn comigi due ssteba cas \n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 1.0\n",
            "Seed: hizo un precioso regalo y le c\n",
            "Texto generado: hizo un precioso regalo y le codzara soenlindan qnlmífo  ca aabsengzadióaain ud\n",
            "dqoaonp yniedtaueveo cosbuedmoagananadoen  soaldo soticra as\n",
            "ualo rs p ml mamho ll smn minmhvtz aosinccnte ssftil\n",
            "i aslté  nsoeraceon un lis fam ciés de vorfeuriro eucrtosaiagce aby iosendaótoe yiñurr e qe e rhesjgo abaltohoio ser ue fo mo cuesrí cvo\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 1.2\n",
            "Seed: hizo un precioso regalo y le c\n",
            "Texto generado: hizo un precioso regalo y le cilóajaa\n",
            "aatlú\n",
            "giveesme eelrvb e lo\n",
            "iaene brc isrm uoaue nn os y  íntasren vesúnlunac s\n",
            "l eusaaeguarra arl á ysgna  an\n",
            "altlo hocsi en oood peeya\n",
            "quvcv\n",
            "mrrarando\n",
            "oun\n",
            "iaciroondorpr lonái aaego aesrluel\n",
            "ajo\n",
            "oyn aenoc s bñrintnn prnerdíl o\n",
            " ql\n",
            " zo oa  bomjain  euerea dnc bri\n",
            "hlc ma  paran e ócaomsijieran\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.2\n",
            "Seed: recharme era para mí un verdad\n",
            "Texto generado: recharme era para mí un verdadero de la basa ae li amo cs clma clere ll coso de la casa ee mu pano di manfervicio de la easeza ce aa cscuadra combinada earcharan ee pabían tonsramio sor la casa ee la crisencia de las cambatientes de la masa al mer me pije e la cosa de la masa don la casa en ea casa ll sin de esto lstao servible \n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.5\n",
            "Seed: recharme era para mí un verdad\n",
            "Texto generado: recharme era para mí un verdadero de aquel a nimilia de  pasmo eir idio de mu poreza s niclar en oa cesto ia don ea lirda de sa  prnas pindabanaas erlco due ea  cart en due eon clteican e an aa mscuadra dorque ro epercanda aosallerotoosos\n",
            "aodos ees sachas cltaraas de masa ye loenaes _anintro n hon hl o e moled de mu loz sn al co\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 1.0\n",
            "Seed: recharme era para mí un verdad\n",
            "Texto generado: recharme era para mí un verdaderé uun óra \n",
            "dis\n",
            "gorida\n",
            "\n",
            "ieouaas e lntetairoosmn tsp dcto\n",
            "m tna eleuae  a doc zsannu cesprcuadra lrpaició asagman o saonso eesui ala eoopcniot ps cuaauació csodmocion ca\n",
            "cirgede upabe\n",
            " i cijo trcs pn onid  periio lnaá\n",
            "eisbd oes debíaientos euasgtuiammo eínairso\n",
            "  r  radaio y_en e veseue civeineór do\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 1.2\n",
            "Seed: recharme era para mí un verdad\n",
            "Texto generado: recharme era para mí un verdaderoe ha desdmpde\n",
            "a  p ágacochtoñ cacaren urisa\n",
            "osu  egmvvadco a pue ean\n",
            "stonserso\n",
            "trs iníeóntl aunisadadagil qunid man erad\n",
            "éóst ml re aemdem fetos \n",
            "ospués pepobdde  sa ovsndcedns labaclas scué taniasiondone  nntltenoeadfa os elcéltsendlimie i iam 1uejhl\n",
            "rer m   d lecncdnarash  eo d rer oe  pashcoha\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.2\n",
            "Seed: opa hacía fuego\n",
            "igualmente sob\n",
            "Texto generado: opa hacía fuego\n",
            "igualmente sobre las cobas de la mscuadra eara lí mi ancontra a gunos plegrías con la vanta de la carina en eascasa dn ea carina en el pambate de la sasa a la caride larar el cube de la prca de pués de li amo el aar ye la  pasas dellas cngleses cn ll combate de la caseta de la  casas de las casas de lo  partes de\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.5\n",
            "Seed: opa hacía fuego\n",
            "igualmente sob\n",
            "Texto generado: opa hacía fuego\n",
            "igualmente sobre mas cactes en eaerto data meseectatte astre lllarsodeo dn psan juan_\n",
            "m dranguado si csbarca ce laistezie la psondcada cl el esantí ana_ ea estresión de las fertes de pefanaando en el pomtramio se mullid lor la binose so pspacadh aor lom tst  nnfertos dtlj se pesamo el _esastello d mu asberonoranl\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 1.0\n",
            "Seed: opa hacía fuego\n",
            "igualmente sob\n",
            "Texto generado: opa hacía fuego\n",
            "igualmente sobaei\n",
            "el mui mosvoapdaqen se seceslisad es\n",
            "vue cltampiy posasp ee celpe au eassa dplca doeeíryaando mispavaeree \n",
            "aác pieeeoeseim p ei vctión re oudir iled\n",
            "eu qos pimhas aenid mue somndo ailan av do  ouclirs s fam ra aroseyizmosan  orefeoeeas oa qilpn qo\n",
            "ce aar\n",
            "a\n",
            " intrrepj venirea ftserpebanos crhiriir\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 1.2\n",
            "Seed: opa hacía fuego\n",
            "igualmente sob\n",
            "Texto generado: opa hacía fuego\n",
            "igualmente sobre idocd te _d osaclsurie a yiss dersofatcima tisf\n",
            "eimda denemta\n",
            "ae  creuron\n",
            "seséne nvásnqurz ornn sarianeeojostosida vepsoae l\n",
            "bo pmtgnuarrn mamppo\n",
            "\n",
            "oazfa ne lepa rcepu2e qasadh aej\n",
            "aráaúnloedonon emtama sv mi y uinaelodite lememác\n",
            "a e \n",
            "lailes rsusad dónrcesp tosa tearieneter cvlrmemar c qúm uivari\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.2\n",
            "Seed: ombate ni se extinguió su geni\n",
            "Texto generado: ombate ni se extinguió su genio de ca mrlte del car con ei amo ee la posa ella canterión de ma casa ella eanterión de la mscuadra combinada ci amo estaba en ea  cosas de las crltes de la carina en el pombate de mi amo con la cica ee las marineros se labían eelvado a la escuadra ce  coerpo de li amo con tl mar y mesaués de la car\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.5\n",
            "Seed: ombate ni se extinguió su geni\n",
            "Texto generado: ombate ni se extinguió su genio me pluellas mabisios le aeesero ee li ala eontcontesta caisr el pombate h sorollero mn lascecenida don eomte de la  cona  errigees de la  mrezas cor ue le\n",
            "cuchonido qrr callendo eas pue eqmculuado d loos d ms _enoonom de laspartra pibidaelrosespuadra englesa ponprendeó pa cgtiedad dn eas gosas pe \n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 1.0\n",
            "Seed: ombate ni se extinguió su geni\n",
            "Texto generado: ombate ni se extinguió su genio\n",
            "srro soma desa meto eievara\n",
            "\n",
            "eealesos due aycerit dsaule a uue\n",
            "lo \n",
            "roertá cubinon e ente nosfaeni salcstedído mllu custe eeroereto yied  px eqn neeoirin cera dí ee  conditnes pnnercióntos\n",
            "aelaaradis pos\n",
            "yue lucro eo vn brsbnoant   eui ena\n",
            "euiroe\n",
            "uan qnresam\n",
            "q les pinrrn\n",
            "cova vn\n",
            "osuel ba castr sls \n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 1.2\n",
            "Seed: ombate ni se extinguió su geni\n",
            "Texto generado: ombate ni se extinguió su genio tuancerlñiucidinlmenzerlas mecon cu lísmisida tajmi cuñíodna mo coanesnb 2e éhiemancuó aedi tió éstsoctiolimo ertatle e a os ruenla sue ieu poestra eeisu inic  tjealy ceosyisñi9adróoodelmamm donooutvmsluin \n",
            "qereiepttu ida \n",
            "dayeabes _asirpn aueczasan ai\n",
            "pibtmat cbarn \n",
            "eeuelulenuesp vui oa es eeaped\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.2\n",
            "Seed: ente hubiera producido en todo\n",
            "Texto generado: ente hubiera producido en todos los ialeces d e la eobeza se sa escuadra eontencia de la mosa sl cobo de la escuadra cara sue el mombate de ma cora al cenos de aa cscuadra eombinada y sllmesso a lontear en el combate d ne amo de la cscuadra en la casa dun ea car enpera ión tara cl ceimer contidtí y ae había con el camento de la \n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.5\n",
            "Seed: ente hubiera producido en todo\n",
            "Texto generado: ente hubiera producido en todos las npiciales ee lue sas dumbates de eceesaban aomo qn pera prano de pas pañones uon sl maimer einfia ee su  trascias  ter ua earha\n",
            "eontían la  crranses ee le hemltros laspsceedito yo sé qs mquellqencada con ectas qnts pe prubar casce  vaarerno due meee  tl eardo ae pi cengre sombrestebmlgmrí pomo\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 1.0\n",
            "Seed: ente hubiera producido en todo\n",
            "Texto generado: ente hubiera producido en todosls roni naano min qmasronda qor listuebos\n",
            "rueitran orarzes pojauseyes sue les qscén jrnto cbuellciei ae musroerud uuérpa loero otvsga nu irmeaseañcidad den sarigra ethl cadtxcutaladvendemfgtirisue lo muimros cacieldo \n",
            "aue quseí hm poeso\n",
            "ar heipo sle ra healboomdoñojo mnbrrí sasflointadidisa  el\n",
            "liu\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 1.2\n",
            "Seed: ente hubiera producido en todo\n",
            "Texto generado: ente hubiera producido en todo enndlnonlecans duro due\n",
            "aásttna yomtto nlocuntión\n",
            "ciateanes el mugtaecerho enid  ejoan fe pbmebeneidos vicpesloma h bentajvos hayrdo zogayuercimmnpofaa as\n",
            "pomoro  obú li guívdgosprntren dn lacanpcar odple oosio ta eaan\n",
            "biepuacanctr comostluestnuíél tose  bd inosim y\n",
            "haemasas s  tannanozdranonaoerta\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.2\n",
            "Seed: ones_ sin duda a causa de su u\n",
            "Texto generado: ones_ sin duda a causa de su una lo  driciales dncontrstables de aa casa ee la cante desla cscuadra ee la pscuadra carqll mar esta o pl par d  antes de la cscuadra conbinada c me pués de la cscuadra ce misamo qe ea casa l la casa ca cana ya canta par ua casa ln ea marina e la corte eesma cscuadra dorqlo tarte desla cosa c las co\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.5\n",
            "Seed: ones_ sin duda a causa de su u\n",
            "Texto generado: ones_ sin duda a causa de su usaed ai hrnoeció ello posita dl cuuella aoestra ys sombate no se cabía cer ee ocsa monseaiturs qo es aue nceedaosabe ca iiro eeda dl carina a ee mur palts de mu ceñblante po  añtaría yesua oeepulabión ae diro a  ee  cariico san ta drerra dellu mpíillería aomo esteya cn mispmo eue no sabía ln ra lltu\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 1.0\n",
            "Seed: ones_ sin duda a causa de su u\n",
            "Texto generado: ones_ sin duda a causa de su usa   yodcáerte eeedsedseods canus amánte dltamaramenrenáan ia aíl tonn muee h pise\n",
            " smigla ton lastir le úesia ilsarara\n",
            "e aproc cttqemblnde fonacde q niroeaa genasa ce  moue  nst eeraila candaevtra roarpo l ui ierina qicld  daro\n",
            "linrde ml semrir pnt rorogeavy liimaehmdololeaicnon lurdasiddes dzen ir\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 1.2\n",
            "Seed: ones_ sin duda a causa de su u\n",
            "Texto generado: ones_ sin duda a causa de su uneey\n",
            "s mu oasad t l8 u|0\n",
            " ecdntalcial dn onets em hbuía ai_ad casrrsa s\n",
            "vrcitanome pesnteotórea sx aara deemjro mesible dabtrardaeldae ecle_\n",
            "\n",
            "muvenesi tl foni a r aunoaídyag eaanaepdrr na rienaoomere  eóm cue ien o porae  hértenee tirbozababae iue\n",
            "tuorés fa  úujirantb4s lúodídodajas ponco  rue\n",
            "sísho\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.2\n",
            "Seed: ra\n",
            "aliada mientras todos los d\n",
            "Texto generado: ra\n",
            "aliada mientras todos los desás en _rayo_ de he ppa lntaba en ll combate d an mambate de ma escuadra don el mamento dn la cabeza de lascarina a la casa al par ne la ear na dn el pombate de  morgo paentosino ee pués de la dasa en ea ear de li amo cabra donstado a la caseza de la carina se la caseta de ma cantesiora a la cara d\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.5\n",
            "Seed: ra\n",
            "aliada mientras todos los d\n",
            "Texto generado: ra\n",
            "aliada mientras todos los des aengos me las nobones de tasmaleza de  sasee te ma eebpnteda astoe lada llmí lespi\n",
            "coppañyebía ee lstodo cn coen rncrnc ae he cepirían cas pficiales m cas boles yo se pesc la hovidnaaión de  _ovío ee hi cetabuardia ee ir eue ee cteuna naanto yi servacéas epeunas csparanzos pera lll_eror de la tal\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 1.0\n",
            "Seed: ra\n",
            "aliada mientras todos los d\n",
            "Texto generado: ra\n",
            "aliada mientras todos los deemceoofuluguin hasfos pe aa  turmoros yoluidsos nrdrdrdn lmmnaiutviail ds salcmla \n",
            "e dernno cn pearoledo nue soevt plqarrpes pue tamieren ai catinolue luanqosa\n",
            "en\n",
            "didir uinni ádepe masnaréc pole vaane sabieato ento puyéaba  ó cá\n",
            "áuosen  ebbs as d yocvieron aaacóbos dar leldrlan cl bastorceedodsoooe\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 1.2\n",
            "Seed: ra\n",
            "aliada mientras todos los d\n",
            "Texto generado: ra\n",
            "aliada mientras todos los drmdriroseqhrceats  haiciszo uutorrmuarpoeuileraosorcx tambo \n",
            "ifo c 5aen ro aczasmn qaspbre le jntesadlalnda xinora  ntaevboalr spponldisnerhayrucrooc ramé  ca dasovam\n",
            " aearecan ela érbuqtnu\n",
            "dl hu \n",
            "puavies fbtncdosas ie aacladyirrible aumiord aomfairascarcl nmataoiz\n",
            "mebmalbri aivoromplsar felatiines \n",
            "\n"
          ]
        }
      ],
      "source": [
        "history=model.fit(X,y, validation_split=0.05, batch_size=128, epochs=10,shuffle=True,verbose=0,callbacks=[generation_callback]).history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBbmz9DMhVhc"
      },
      "source": [
        "## Entregable\n",
        "\n",
        "Completa los apartados anteriores para entrenar modelos del lenguaje que sean capaces de generar texto con cierto sentido. Comentar los resultados obtenidos y cómo el modelo va mejorando época a época. Comentar las diferencias apreciadas al utilizar diferentes valores de temperatura. Entregar al menos la salida de un entrenamiento completo con los textos generados época a época.\n",
        "\n",
        "El objetivo no es conseguir generar pasajes literarios con coherencia, sino obtener lenguaje que se asemeje en cierta manera a lo visto en el texto original y donde las palabras sean reconocibles como construcciones en castellano. Como ejemplo de lo que se puede conseguir, este es el resultado de generar texto después de 10 epochs y con temperature 0.2 usando El Quijote:\n",
        "\n",
        "\n",
        "```\n",
        "-----> Epoch: 10 - Generando texto con temperature 0.2\n",
        "Seed: o le cautivaron y rindieron el\n",
        "Texto generado: o le cautivaron y rindieron el caballero de la caballería de la mano de la caballería del cual se le dijo:\n",
        "\n",
        "-¿quién es el verdad de la caballería de la caballería de la caballería de la caballería de la caballería, y me ha de habían de la mano que el caballero de la mano de la caballería. y que no se le habían de la mano de la c\n",
        "\n",
        "```\n",
        "\n",
        "Asimismo, se proponen los siguientes aspectos opcionales para conseguir nota extra:\n",
        "\n",
        "*   Experimentar con los textos de teatro en verso de Calderón de la Barca (¿es capaz el modelo de aprender las estructuras del teatro en verso?) o con alguno de los otros textos disponibles. También se puede probar con textos de vuestra elección.\n",
        "*   Experimentar con distintos valores de SEQ_LENGTH.\n",
        "*   Experimentar con los hiperparámetros del modelo o probar otro tipo de modelos como GRUs o *stacked* RNNs (RNNs apiladas).\n",
        "*   Experimentar utilizando embeddings en vez de representaciones one-hot.\n",
        "*   (Difícil) Entrenar un modelo secuencia a secuencia en vez de secuencia a carácter.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}